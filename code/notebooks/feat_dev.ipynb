{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Development Extraction: Neurophysiology [dyskinesia project]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<b> Content </b>\n",
    "\n",
    "\n",
    "<b> Aperiodic estimates </b>\n",
    "Relevant literature:\n",
    "- Periodic and a-periodic components relevance and interaction, different reasons (per + a-per) for signal changes observed within a specific bandwidth. Aperiodic component (complicated) vs exponent (1/f) (Donoghue, ..., Shestyuk & Voytek, Nature Neurosc 2020 : https://www.nature.com/articles/s41593-020-00744-x)\n",
    "- cycle-by-cycle features: bycycle toolbox (Cole & Voytek, J of Neurophys 2019, https://journals.physiology.org/doi/full/10.1152/jn.00273.2019)\n",
    "- aperiodic component, PD severity, and cortico-subcortico-activity, Bush & Zou, Richardson, bioRxiv 2023 https://www.biorxiv.org/content/10.1101/2023.02.08.527719v1?rss=1\n",
    "\n",
    "<b> Periodic component analysis: </b> \n",
    "- Try Wavelet Dceomposition vs Welch (tapered) Spectral Decomposition\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Loading packages and functions, defining paths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python and external packages\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import csv\n",
    "from dataclasses import dataclass, field, fields\n",
    "from collections import namedtuple\n",
    "from typing import Any\n",
    "from itertools import compress\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from scipy.stats import pearsonr, mannwhitneyu\n",
    "import statsmodels as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from  matplotlib import __version__ as plt_version\n",
    "from  scipy import __version__ as scipy_version\n",
    "from scipy import signal, stats\n",
    "# from array import array\n",
    "# import datetime as dt\n",
    "# #mne\n",
    "# import mne_bids\n",
    "# import mne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python sys 3.9.0 (default, Nov 15 2020, 08:30:55) [MSC v.1916 64 bit (AMD64)]\n",
      "pandas 1.4.4\n",
      "numpy 1.23.3\n",
      "sci-py 1.9.3\n",
      "sci-kit learn 1.1.3\n",
      "matplotlib 3.5.3\n",
      "statsmodels 0.14.0\n"
     ]
    }
   ],
   "source": [
    "# check some package versions for documentation and reproducability\n",
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "# print('mne_bids', mne_bids.__version__)\n",
    "# print('mne', mne.__version__)\n",
    "print('sci-py', scipy_version)\n",
    "print('sci-kit learn', sk.__version__)\n",
    "print('matplotlib', plt_version)\n",
    "print('statsmodels', sm.__version__)\n",
    "## DEC 11 2022:\n",
    "# Python sys 3.9.0 (default, Nov 15 2020, 08:30:55)\n",
    "# pandas 1.4.4\n",
    "# numpy 1.23.3\n",
    "# sci-py 1.9.3\n",
    "# sci-kit learn 1.1.3\n",
    "# matplotlib 3.5.3\n",
    "\n",
    "## Updated ENV DEC 11 2022:\n",
    "# Python sys 3.10.8  (main, Nov  4 2022, 13:42:51)\n",
    "# pandas 2.1.4  -> downgraded to pandas 1.5.3 bcs of pickle functionality (pd.indexing error)\n",
    "# numpy 1.26.2\n",
    "# sci-py 1.11.4\n",
    "# sci-kit learn 1.3.2\n",
    "# matplotlib 3.8.2\n",
    "# statsmodels 0.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_project_path_in_notebook(\n",
    "    subfolder: str = '',\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds path of projectfolder from Notebook.\n",
    "    Start running this once to correctly find\n",
    "    other modules/functions\n",
    "    \"\"\"\n",
    "    path = os.getcwd()\n",
    "\n",
    "    while path[-20:] != 'dyskinesia_neurophys':\n",
    "\n",
    "        path = os.path.dirname(path)\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define local storage directories\n",
    "projectpath = get_project_path_in_notebook()\n",
    "codepath = os.path.join(projectpath, 'code')\n",
    "figpath = os.path.join(projectpath, 'figures')\n",
    "datapath = os.path.join(projectpath, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(codepath)\n",
    "# own utility functions\n",
    "import utils.utils_fileManagement as utilsFiles\n",
    "import utils.utils_windowing as utilsWindows\n",
    "from utils.utils_fileManagement import (get_project_path,\n",
    "                                        load_class_pickle,\n",
    "                                        save_class_pickle,\n",
    "                                        mergedData,\n",
    "                                        correct_acc_class)\n",
    "# own data preprocessing functions\n",
    "import lfpecog_preproc.preproc_data_management as dataMng\n",
    "import lfpecog_preproc.preproc_filters as fltrs\n",
    "# own data exploration functions\n",
    "import lfpecog_features.feats_read_proc_data as read_data\n",
    "import lfpecog_features.feats_spectral_baseline as specBase\n",
    "import lfpecog_features.feats_spectral_features as spectral\n",
    "import lfpecog_features.feats_spectral_helpers as specHelp\n",
    "import lfpecog_features.feats_helper_funcs as ftHelp\n",
    "\n",
    "import lfpecog_preproc.preproc_import_scores_annotations as importClin\n",
    "import lfpecog_analysis.import_ephys_results as importResults\n",
    "# import lfpecog_analysis.get_acc_derivs as accDerivs\n",
    "\n",
    "\n",
    "import lfpecog_plotting.plotHelpers as plotHelp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load preprocessed and merged Sub-Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load ACC pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_plot = [\n",
    "    '008', \n",
    "    # '009', '010', '012', \n",
    "    # '013', '014', '016'\n",
    "]\n",
    "\n",
    "data_version = 'v4.0'\n",
    "mins_recording = []\n",
    "\n",
    "for sub in subs_to_plot:\n",
    "    # load Acc-detected movement labels\n",
    "    acc = load_class_pickle(os.path.join(\n",
    "        get_project_path('data'),\n",
    "        'merged_sub_data', data_version, f'sub-{sub}',\n",
    "        f'{sub}_mergedData_{data_version}_accLeft.P'\n",
    "    ))\n",
    "    # acc = correct_acc_class(acc)\n",
    "\n",
    "    # mins_recording.append(acc.data.shape[0] / acc.fs / 60)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load ephys pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WINDOWED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_to_plot = [\n",
    "    # '008', '009', '010', '012', \n",
    "    # '013', '014',\n",
    "    '012',\n",
    "]\n",
    "task = 'rest'\n",
    "data_version = 'v3.0'\n",
    "mins_recording = []\n",
    "\n",
    "# for sub in subs_to_plot:\n",
    "#     # load Acc-detected movement labels\n",
    "#     data = load_class_pickle(os.path.join(\n",
    "#         get_project_path('data'), 'windowed_data_classes_60s',\n",
    "#         data_version, f'sub-{sub}', task,\n",
    "#         f'{sub}_mneEpochs_{task}_{data_version}_win60s_overlap0.5.P'\n",
    "#     ))\n",
    "#     # acc = correct_acc_class(acc)\n",
    "\n",
    "# data.list_mne_objects[0].times.shape\n",
    "# data.list_mne_objects[0].ch_names\n",
    "# data.list_mne_objects[0].get_data().shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MERGED DATA per DATATYPE (source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD TASK DATA BEFORE MERGING\n",
    "sub = '019'\n",
    "data_version = 'v4.0'\n",
    "dType = 'ecog_right'\n",
    "\n",
    "path = os.path.join(get_project_path('data'),\n",
    "                    'preprocessed_data',\n",
    "                    f'sub-{sub}', 'v4.0',)\n",
    "f_data = f'data_{sub}_Rest_StimOffDopa35_{data_version}_{dType}_2048Hz.npy'\n",
    "c_data = f'names_{sub}_Rest_StimOffDopa35_{data_version}_{dType}.csv'\n",
    "\n",
    "d = np.load(os.path.join(path, f_data))\n",
    "cols = pd.read_csv(os.path.join(path, c_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MERGED PICKLE\n",
    "\n",
    "data_version = 'v4.0'\n",
    "sub = '019'\n",
    "dat = {}\n",
    "dType = 'lfp_right'\n",
    "dat[dType] = load_class_pickle(os.path.join(\n",
    "        get_project_path('data'), 'merged_sub_data',\n",
    "        'v4.0', f'sub-{sub}',\n",
    "        f'{sub}_mergedData_{data_version}_{dType}.P'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_sources = ['lfp_right', 'lfp_left' ]  #'lfp_right', 'ecog_left', 'ecog_right']\n",
    "use_stored_windows = True\n",
    "\n",
    "sub = '108'\n",
    "json_path = os.path.join(utilsFiles.get_onedrive_path('data'),\n",
    "                     'featureExtraction_jsons',\n",
    "                     'ftExtr_spectral_v6.json')\n",
    "with open(json_path, 'r') as json_data:\n",
    "    SETTINGS = json.load(json_data)\n",
    "\n",
    "windows_path = os.path.join(utilsFiles.get_project_path('data', extern_HD=False),\n",
    "                    'windowed_data_classes_'\n",
    "                    f'{SETTINGS[\"WIN_LEN_sec\"]}s_'\n",
    "                    f'{SETTINGS[\"WIN_OVERLAP_part\"]}overlap',\n",
    "                    SETTINGS['DATA_VERSION'],\n",
    "                    f'sub-{sub}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = {}\n",
    "for dType in ephys_sources[:1]:\n",
    "    print(f'\\tstart {dType}')\n",
    "    # define path for windows of dType\n",
    "    dType_fname = (f'sub-{sub}_windows_'\n",
    "                    f'{SETTINGS[\"WIN_LEN_sec\"]}s_'\n",
    "                    f'{SETTINGS[\"DATA_VERSION\"]}_{dType}.P')\n",
    "    dType_win_path = os.path.join(windows_path, dType_fname)\n",
    "\n",
    "    # check if windows are already available\n",
    "    if np.logical_and(use_stored_windows,\n",
    "                      os.path.exists(dType_win_path)):\n",
    "        print(f'load data from {windows_path}....')\n",
    "        # windows[dType] = utilsFiles.load_class_pickle(dType_win_path)\n",
    "        wins = utilsFiles.load_class_pickle(dType_win_path)\n",
    "        print(f'\\tWINDOWS LOADED from {dType_fname} in {windows_path}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore ssd'd Spectral Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lfpecog_features.get_ssd_data as ssd\n",
    "import lfpecog_analysis.ft_processing_helpers as ftProc\n",
    "import lfpecog_analysis.get_SSD_timefreqs as ssd_TimeFreq\n",
    "import lfpecog_plotting.plot_timeFreqs_ssd_psds as plot_ssd_TFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot SSD construction overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT VISUAL SSD OVERVIEW\n",
    "\n",
    "# # loop over defined frequency bands\n",
    "# for bw in SETTINGS['SPECTRAL_BANDS']:\n",
    "#     f_range = SETTINGS['SPECTRAL_BANDS'][bw]\n",
    "    # check whether to perform SSD\n",
    "    # (ssd_filt_data,\n",
    "    #     ssd_pattern,\n",
    "    #     ssd_eigvals\n",
    "    # ) = ssd.get_SSD_component(\n",
    "    #     data_2d=win_dat.T,\n",
    "    #     fband_interest=f_range,\n",
    "    #     s_rate=windows.fs,\n",
    "    #     use_freqBand_filtered=True,\n",
    "    #     return_comp_n=0,\n",
    "    # )\n",
    "\n",
    "#     f, psd = signal.welch(ssd_filt_data, axis=-1,\n",
    "#                     nperseg=windows.fs, fs=windows.fs)\n",
    "#     plt.plot(f, psd, label=bw)\n",
    "\n",
    "        \n",
    "\n",
    "# plt.xlim(0, 100)\n",
    "# plt.title(f'WINDOW # {i_w} - {dType.upper()}')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SSD timeseries (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get windowed bands of different dtypes per sub\n",
    "importlib.reload(ftHelp)\n",
    "importlib.reload(ssd)\n",
    "importlib.reload(ssd_TimeFreq)\n",
    "\n",
    "sub = '022'\n",
    "\n",
    "# call from feats_extract_multivar.py\n",
    "ssd_subClass = ssd.get_subject_SSDs(\n",
    "    sub=sub,\n",
    "    incl_stn=True,\n",
    "    incl_ecog=True,\n",
    "    ft_setting_fname='ftExtr_spectral_v6.json',)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = ssd_subClass.lfp_left.hi_beta\n",
    "print(dat.shape)\n",
    "times = np.array(ssd_subClass.lfp_left.times)\n",
    "print(len(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAN_ACCEPT = 0.1\n",
    "c = 0\n",
    "for i, d in enumerate(dat):\n",
    "    if any(np.isnan(d)) and (sum(np.isnan(d)) / len(d)) <= NAN_ACCEPT:\n",
    "        d = dat[i].copy()\n",
    "        # d[np.isnan(d)] = 0\n",
    "        # d[~np.isnan(d)] = np.nan\n",
    "        plt.plot(d + 10*c)\n",
    "        c += 1\n",
    "\n",
    "print(f'# of windows with (accepted) NaNs: {c} (out of {len(dat)})')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore feature - dyskinesia correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature details\n",
    "WIN_LEN=10\n",
    "WIN_OVERLAP=0.5\n",
    "DATA_VERSION='v4.0'\n",
    "FT_VERSION='v6'\n",
    "SSD_BROAD=True\n",
    "INCL_STN_ONLY_PTS=True\n",
    "IGNORE_PTS=['011', '104', '106']\n",
    "\n",
    "# analysis details\n",
    "EPOCH_LEN_SEC = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find available subs\n",
    "SUBS = utilsFiles.get_avail_ssd_subs(\n",
    "    DATA_VERSION=DATA_VERSION,\n",
    "    FT_VERSION=FT_VERSION,\n",
    "    IGNORE_PTS=IGNORE_PTS\n",
    ")\n",
    "print(f'SUBS: n={len(SUBS)} ({SUBS})')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lfpecog_plotting.plot_descriptive_SSD_PSDs as plot_PSDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(ssd_TimeFreq)\n",
    "\n",
    "\n",
    "TFs = ssd_TimeFreq.get_all_ssd_timeFreqs(\n",
    "    SUBS=SUBS, FT_VERSION=FT_VERSION,\n",
    "    DATA_VERSION=DATA_VERSION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lfpecog_plotting.prep_and_plot_groupTimeFreqs as groupTFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(groupTFs)\n",
    "source = 'lfp_right'\n",
    "FIG_TARGET = 'LID'\n",
    "\n",
    "(ps_arr, arr_times,\n",
    " fx, present_subs) = groupTFs.get_group_timefreq(\n",
    "    source=source, TARGET=FIG_TARGET,\n",
    "    MIN_SUBS=5, DATA_VERSION=DATA_VERSION,\n",
    "    FT_VERSION=FT_VERSION, LOG_POWER=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(groupTFs)\n",
    "\n",
    "SHOW_PRES_SUBS = True\n",
    "\n",
    "FIG_NAME = f'timeFreq_{FIG_TARGET}_{source}'\n",
    "if SHOW_PRES_SUBS: FIG_NAME += '_subs'\n",
    "\n",
    "groupTFs.plot_group_timeFreq(\n",
    "    TARGET=FIG_TARGET, SHOW_PRES_SUBS=SHOW_PRES_SUBS,\n",
    "    data_arr=ps_arr, freqs=fx, times=arr_times,\n",
    "    present_subs=present_subs,\n",
    "    save_figname=FIG_NAME,\n",
    "    FT_VERSION=FT_VERSION,\n",
    "    DATA_VERSION=DATA_VERSION,\n",
    "    source=source,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check SSD extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replicate SSD extraction\n",
    "\n",
    "importlib.reload(ssd)\n",
    "# loop over windows\n",
    "\n",
    "for i_w, win_dat in enumerate(windows.data[:5]):\n",
    "    win_dat = win_dat.astype(np.float64)    \n",
    "    # select only rows without missing\n",
    "    nan_rows = np.array([pd.isna(win_dat[:, i]).any()\n",
    "                for i in range(win_dat.shape[-1])])\n",
    "    win_dat = win_dat[:, ~nan_rows]\n",
    "    win_chnames = list(compress(windows.keys, ~nan_rows))\n",
    "    win_time = windows.win_starttimes[i_w]\n",
    "    \n",
    "    ssds = ssd.SSD_bands_per_window(\n",
    "        data=win_dat.T, s_rate=windows.fs,\n",
    "        freq_bands_incl=SETTINGS['SPECTRAL_BANDS'],\n",
    "    )\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne_connectivity as mne_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_conn.__version__  # requires newer version for creation of conn results\n",
    "# additional dependencies: trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported new connectivity functions \n",
    "from lfpecog_analysis._connectivity_helpers import (\n",
    "    load_coordinates,\n",
    "    process_results,\n",
    "    plot_results_timefreqs,\n",
    "    plot_results_patterns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing straight from results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conn_values_sub_side(\n",
    "    results_df, sub, stn_side, conn_method,\n",
    "):\n",
    "    # if conn_method == 'mic':\n",
    "    conn_data = [\n",
    "        results_df.iloc[i][conn_method]\n",
    "        for i in np.arange(results_df.shape[0])\n",
    "        if (results_df.iloc[i]['subject'] == sub and\n",
    "            results_df.iloc[i]['seed_target_lateralisation'] == stn_side)\n",
    "    ][0]\n",
    "\n",
    "    return conn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conn_timefreq(\n",
    "    plot_arr, plot_freqs, plot_times,\n",
    "    conn_method, title=None,\n",
    "):\n",
    "\n",
    "    fig, axis = plt.subplots(1, 1, figsize=(8, 4))\n",
    "\n",
    "    if conn_method == 'trgc': vmin, vmax = (-1, 1)\n",
    "    else: vmin, vmax = None, None\n",
    "\n",
    "    image = axis.imshow(\n",
    "        plot_arr.T,\n",
    "        origin=\"lower\",\n",
    "        extent=(\n",
    "            plot_times[0] / 60,\n",
    "            plot_times[-1] / 60,\n",
    "            plot_freqs[0],\n",
    "            plot_freqs[-1],\n",
    "        ),\n",
    "        vmin=vmin, vmax=vmax,\n",
    "        aspect=\"auto\",\n",
    "        cmap=\"viridis\",\n",
    "    )\n",
    "    axis.set_xlabel(\"Time (minutes)\")\n",
    "    axis.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_axis = fig.add_axes([0.88, 0.15, 0.02, 0.7])\n",
    "    fig.colorbar(image, cax=cbar_axis, label=\"Connectivity (A.U.)\",)\n",
    "\n",
    "    # title = f\"Method: {METHOD}, sub {sub}, ECoG * {stn_side}-STN\"\n",
    "    if title is not None: axis.set_title(title)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = utilsFiles.load_ft_ext_cfg(FT_VERSION='v6')\n",
    "\n",
    "CONN_FT_PATH = os.path.join(\n",
    "    utilsFiles.get_project_path('results'),\n",
    "    'features',\n",
    "    'connectivity',\n",
    "    (f'windows_{SETTINGS[\"WIN_LEN_sec\"]}s_'\n",
    "     f'{SETTINGS[\"WIN_OVERLAP_part\"]}overlap')\n",
    ")\n",
    "\n",
    "# take only subjects with ECoG & LFP data\n",
    "SUBJECTS = [sub for sub in SETTINGS['TOTAL_SUBS']\n",
    "            if sub.startswith(\"0\")]\n",
    "\n",
    "METHOD = \"mic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, window_times, freqs = process_results(\n",
    "    method=METHOD, subjects='008', results_path=CONN_FT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())\n",
    "print(len(window_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all time-freqs with Connectivity values\n",
    "sub = '012'\n",
    "stn_side = 'ipsilateral'\n",
    "\n",
    "for sub in SUBJECTS:\n",
    "    for stn_side in ['contralateral', 'ipsilateral']:\n",
    "\n",
    "        temp_dat = get_conn_values_sub_side(\n",
    "            results_df=results, sub=sub,\n",
    "            stn_side=stn_side, conn_method=METHOD\n",
    "        )\n",
    "\n",
    "        # plot_conn_timefreq(plot_arr=temp_dat,\n",
    "        #            plot_times=window_times,\n",
    "        #            plot_freqs=freqs,\n",
    "        #            conn_method=METHOD,\n",
    "        #            title=f'{METHOD.upper()}: sub {sub}, {stn_side}')\n",
    "\n",
    "\n",
    "# PM:\n",
    "# # average over multiple subs and/or sides\n",
    "# temp_dat = [\n",
    "#     get_conn_values_sub_side(\n",
    "#         results_df=results, sub=SUB,\n",
    "#         stn_side=stn_side, conn_method=METHOD\n",
    "#     ) for SUB in SUBJECTS[:5]\n",
    "# ]\n",
    "# temp_dat = np.nanstd(temp_dat, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting via new scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lfpecog_analysis.process_connectivity as processConn\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_arrays(ipsivalues, ipsi_ids):\n",
    "    stat_values, stat_ids = {}, {}\n",
    "    for lid_code, lid_cats in zip(['lid', 'nolid'],\n",
    "                                    [[1, 2, 3], [0]]):\n",
    "        # select all arrays LID vs NO-LID, sorted in groups of category\n",
    "        # [sub1-cat1, sub2-cat1, .., subX-cat1, sub1-cat2, sub2-cat2, ..]\n",
    "        temp_values = [a for cat in ipsivalues\n",
    "                        for a in ipsivalues[cat] if cat in lid_cats]\n",
    "        # get corresponding list of sub-ids\n",
    "        temp_ids = [s for cat in ipsi_ids for s in ipsi_ids[cat]\n",
    "                        if cat in lid_cats]  # get sub-ids w/o LID\n",
    "        temp_ids = [[s] * a.shape[0] for s, a in\n",
    "                        zip(temp_ids, temp_values)]  # multiply ids with corr array shapes\n",
    "        temp_ids = np.array([i for l in temp_ids for i in l])  # unpack list of lists    \n",
    "        # convert into long arrays\n",
    "        temp_values = np.array([r for a in temp_values for r in a])\n",
    "        assert temp_values.shape[0] == temp_ids.shape[0], 'incorrect shapes'\n",
    "        stat_values[lid_code] = temp_values\n",
    "        stat_ids[lid_code] = temp_ids\n",
    "    # create single arrays for stats\n",
    "    stat_values = np.concatenate([stat_values['nolid'],\n",
    "                                stat_values['lid']])\n",
    "    stat_labels = np.concatenate([[0] * len(stat_ids['nolid']),\n",
    "                                [1] * len(stat_ids['lid'])])\n",
    "    stat_ids = np.concatenate([stat_ids['nolid'],\n",
    "                            stat_ids['lid']])\n",
    "\n",
    "    assert (stat_values.shape[0] == stat_labels.shape[0]\n",
    "            == stat_ids.shape[0]), 'incorrect lengths'\n",
    "\n",
    "    return stat_values, stat_labels, stat_ids\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mixEff_wGroups(dep_var, indep_var,\n",
    "                       groups, TO_ZSCORE=True,\n",
    "                       ALPHA=.01, RETURN_CI=False,\n",
    "                       RETURN_GRADIENT=False,):\n",
    "    \"\"\"\n",
    "    # tests sign effect of LID on ephys\n",
    "    # Model: https://www.statsmodels.org/stable/generated/statsmodels.regression.mixed_linear_model.MixedLM.html\n",
    "    # Results: https://www.statsmodels.org/stable/generated/statsmodels.regression.mixed_linear_model.MixedLMResults.html\n",
    "    \"\"\"\n",
    "\n",
    "    # z-score ephys values on group level for scaling\n",
    "    dep_var = (dep_var - np.std(dep_var)) / np.mean(dep_var)\n",
    "    # define model\n",
    "    lm_model = MixedLM(\n",
    "        endog=dep_var,  # dependent variable (ephys score)\n",
    "        exog=indep_var,  # independent variable (i.e., LID presence, movement)\n",
    "        groups=groups,  # subjects\n",
    "        exog_re=None,  # (None)  defaults to a random intercept for each group\n",
    "    )\n",
    "    # run and fit model\n",
    "    lm_results = lm_model.fit()\n",
    "    # extract results\n",
    "    fixeff_cf = lm_results._results.fe_params[0]\n",
    "    pval = lm_results._results.pvalues[0]\n",
    "\n",
    "    output_list = [fixeff_cf, pval]  # to keep output number dynamic\n",
    "\n",
    "    if RETURN_CI:\n",
    "        conf_int = lm_results.conf_int(alpha=ALPHA)[0]\n",
    "        output_list.append(conf_int)\n",
    "    \n",
    "    if RETURN_GRADIENT:\n",
    "        grad = lm_results._results.params_object()\n",
    "        output_list.append(grad)\n",
    "\n",
    "    # return two, three, or four values\n",
    "    return output_list\n",
    "    \n",
    "    # print(f'fixed effect coeff: {fixeff_cf}')  # fixed-effect coeffs\n",
    "    # print(f'Confidence Interval (alpha: {ALPHA}): {conf_int} (p = {pval.round(5)})')\n",
    "    # print(lm_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_lmem_freqCoeffs(temp_values,\n",
    "                         temp_ids,\n",
    "                         temp_freqs):\n",
    "\n",
    "    # select freqs 4 - 35 Hz and 60 - 90 Hz\n",
    "    f_sel = np.logical_or(\n",
    "        [f >= 4 and f <= 35 for f in temp_freqs],\n",
    "        [f >= 60 and f <= 90 for f in temp_freqs]\n",
    "    )\n",
    "    # correct alpha for multiple comparisons\n",
    "    ALPHA = .05 / sum(f_sel)\n",
    "    print(f'corrected ALPHA within LMEM: {round(ALPHA, 5)} rounded')\n",
    "\n",
    "    # organize long arrays with all values, labels and sub-ids\n",
    "    (stat_values,\n",
    "    stat_labels,\n",
    "    stat_ids) = get_stats_arrays(ipsivalues=temp_values,\n",
    "                                 ipsi_ids=temp_ids)\n",
    "\n",
    "    # calculate coeffs and pvalues per frequency bin\n",
    "    coeffs_freqs, sign_freqs, grads = [], [], []\n",
    "    for i_f, f in enumerate(temp_freqs):\n",
    "        # skip irrelevant freqs\n",
    "        if not f in temp_freqs[f_sel]: continue\n",
    "        # calculate coeffs for med-effect on values (random slopes for subjects)\n",
    "        fixEff_cf, pval, grad = run_mixEff_wGroups(\n",
    "            dep_var=stat_values[:, i_f],\n",
    "            indep_var=stat_labels,\n",
    "            groups=stat_ids,\n",
    "            RETURN_GRADIENT=True,\n",
    "        )\n",
    "        sig_bool = pval < ALPHA\n",
    "        coeffs_freqs.append(fixEff_cf)\n",
    "        sign_freqs.append(sig_bool)\n",
    "        grads.append(grad)\n",
    "\n",
    "    coeffs_freqs = np.array(coeffs_freqs)\n",
    "    sign_freqs = np.array(sign_freqs)\n",
    "    grads = np.array(grads)\n",
    "\n",
    "    return coeffs_freqs, sign_freqs, grads, temp_freqs[f_sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(processConn)\n",
    "importlib.reload(ssd_TimeFreq)\n",
    "\n",
    "FT_VERSION='v6'\n",
    "SETTINGS = utilsFiles.load_ft_ext_cfg(FT_VERSION=FT_VERSION)\n",
    "\n",
    "# take only subjects with ECoG & LFP data\n",
    "SUBJECTS = [sub for sub in SETTINGS['TOTAL_SUBS']\n",
    "            if sub.startswith(\"0\")]\n",
    "DATA_VERSION = SETTINGS['DATA_VERSION']\n",
    "\n",
    "# ### get values\n",
    "# TFs = ssd_TimeFreq.get_all_ssd_timeFreqs(\n",
    "#     SUBS=SUBJECTS,\n",
    "#     FT_VERSION=FT_VERSION,\n",
    "#     DATA_VERSION=DATA_VERSION,\n",
    "#     # GET_CONNECTIVITY='mic',\n",
    "#     verbose=False,\n",
    "# )\n",
    "print('got tf values')\n",
    "\n",
    "# TODO: adjust code for PSDs\n",
    "# connectivity results in TFs [ipsi / contralat]\n",
    "# PSDs: TFs [lfp_left / lfp_right/ ecog_side]\n",
    "# merge lfp left and right into pdSeries on index\n",
    "# take mean -> process similar\n",
    "# ecog, no averaging needed\n",
    "\n",
    "### sort and average values into categories\n",
    "# plot values is dict: contains sides\n",
    "# side dict: contains different categories 0,1,2,3\n",
    "plot_values, freqs, value_ids = processConn.get_conn_values_to_plot(\n",
    "    TFs,\n",
    "    RETURN_MEAN_per_CAT=False,\n",
    "    BASELINE_CORRECT=False,\n",
    "    BASELINE_EXCL_MOVE=True,\n",
    "    CDRS_SIDE='bilat',\n",
    "    INCL_CORE_CDRS=True,\n",
    "    incl_conn_sides=['lfp_left'],\n",
    "    SELECT_MOVEMENT=False,\n",
    "    # verbose=self.VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take data for correct side\n",
    "side = 'lfp_left'\n",
    "side_values = plot_values[side]\n",
    "side_ids = value_ids[side]\n",
    "\n",
    "coeffs_freqs, sign_freqs, grads, freqlist_coeffs = calc_lmem_freqCoeffs(\n",
    "    temp_values=side_values,\n",
    "    temp_ids=side_ids,\n",
    "    temp_freqs=freqs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase explorations\n",
    "\n",
    "Phase features to add:\n",
    "- local PAC from De Hempt (ECoG) (beta-phase, gamma-ampl)\n",
    "    - EEGLAB ASYMM PAC\n",
    "    - check calculation via entropy of amplitudes per bin\n",
    "    - or MI-inde\n",
    "- phase-phase: CHECK CAGNAN BURST WORK\n",
    "    - connectivity phase differences from Swann et al (phase-coherence) (angle STN versus angle ECoG, compare with imaginary-coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lfpecog_plotting.phase_plotting as phaseplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(phaseplot)\n",
    "\n",
    "### phase difference\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# get both signals\n",
    "sig1 = ssds.lo_beta.copy()\n",
    "sig2 = ssd0.lo_beta.copy()\n",
    "# convert to analytic signal\n",
    "a1 = signal.hilbert(x=sig1,)\n",
    "a2 = signal.hilbert(x=sig2,)\n",
    "# get phase from analytical signal, convert from pi to degree\n",
    "rad1 = np.angle(a1)\n",
    "deg1 = np.rad2deg(rad1)\n",
    "rad2 = np.angle(a2)\n",
    "deg2 = np.rad2deg(rad2)\n",
    "# get difference, convert all to positive degrees (-90 -> +270)\n",
    "rad_diff = rad1 - rad2\n",
    "deg_diff = deg1 - deg2\n",
    "mask_deg = deg_diff < 0  # bool-array, 0 for values >= 0\n",
    "mask_rad = rad_diff < 0\n",
    "corr_rad = np.array([2 * np.pi] * len(rad_diff)) * mask_rad  # corr array is set 0 for pos diff-values\n",
    "rad_diff += corr_rad\n",
    "corr_deg = np.array([360] * len(deg_diff)) * mask_deg  # corr array is set 0 for pos diff-values\n",
    "deg_diff += corr_deg\n",
    "plt.plot(rad_diff, label='corr', alpha=.8, ls='dotted')\n",
    "plt.xlim(0, 5000)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sig1)\n",
    "plt.plot(sig2)\n",
    "plt.xlim(0, 5000)\n",
    "plt.show()\n",
    "\n",
    "phaseplot.plot_rose_axis(radians=rad_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_phase = ssd_014.ecog_right.lo_beta[10].copy()\n",
    "sig_ampl = ssd_014.ecog_right.broad_gamma[10].copy()\n",
    "# sig = ssds.lo_beta.copy()\n",
    "\n",
    "a_phase = signal.hilbert(x=sig_phase,)\n",
    "a_ampl = signal.hilbert(x=sig_ampl,)\n",
    "phase = np.angle(a_phase)\n",
    "phase_deg = np.rad2deg(phase)  #phase * (180 / np.pi)\n",
    "ampl = abs(a_ampl)\n",
    "plt.plot(phase)\n",
    "plt.plot(ampl)\n",
    "plt.xlim(0, 1000)\n",
    "plt.yticks([-np.pi, 0, np.pi],\n",
    "           labels=['-180', '0', '+180'],)\n",
    "plt.ylabel('Phase (degree)')\n",
    "plt.xlabel('time (samples)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate PAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorpac import Pac\n",
    "import lfpecog_features.feats_phase_amp_coupling as fts_pac\n",
    "import lfpecog_features.extract_ssd_features as ssdFts\n",
    "from lfpecog_features import bursts_funcs as bursts_funcs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test low-freq vs Beta/gamma hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR while pickle loading of D://Research/CHARITE/projects/dyskinesia_neurophys/data/windowed_data_classes_10s_0.5overlap\\selected_ephys_classes_all\\ephys_selections_105.P\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D://Research/CHARITE/projects/dyskinesia_neurophys/data/windowed_data_classes_10s_0.5overlap\\\\selected_ephys_classes_all\\\\ephys_selections_105.P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\habetsj\\Research\\projects\\dyskinesia_neurophys\\code\\utils\\utils_fileManagement.py:286\u001b[0m, in \u001b[0;36mload_class_pickle\u001b[1;34m(file_to_load, convert_float_np64)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    287\u001b[0m         output \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D://Research/CHARITE/projects/dyskinesia_neurophys/data/windowed_data_classes_10s_0.5overlap\\\\selected_ephys_classes_all\\\\ephys_selections_105.P'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m SUB \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m105\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     10\u001b[0m picklename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mephys_selections_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSUB\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.P\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m sub_class \u001b[38;5;241m=\u001b[39m \u001b[43mload_class_pickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_to_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpicklepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mpicklename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_float_np64\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mvars\u001b[39m(sub_class\u001b[38;5;241m.\u001b[39mlfp_left_INVOLUNTARY_moveboth_lidlid)\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(sub_class\u001b[38;5;241m.\u001b[39mlfp_left_INVOLUNTARY_moveboth_lidlid\u001b[38;5;241m.\u001b[39mtime_arr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\habetsj\\Research\\projects\\dyskinesia_neurophys\\code\\utils\\utils_fileManagement.py:291\u001b[0m, in \u001b[0;36mload_class_pickle\u001b[1;34m(file_to_load, convert_float_np64)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERROR while pickle loading of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_to_load\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_to_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    292\u001b[0m         output \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m    293\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D://Research/CHARITE/projects/dyskinesia_neurophys/data/windowed_data_classes_10s_0.5overlap\\\\selected_ephys_classes_all\\\\ephys_selections_105.P'"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = 'D://Research/CHARITE/projects/dyskinesia_neurophys/data/'\n",
    "picklepath = os.path.join(\n",
    "    data_path,\n",
    "    'windowed_data_classes_10s_0.5overlap',\n",
    "    'selected_ephys_classes_all'\n",
    ")\n",
    "\n",
    "SUB = '105'\n",
    "\n",
    "picklename = f'ephys_selections_{SUB}.P'\n",
    "\n",
    "sub_class = load_class_pickle(\n",
    "    file_to_load=os.path.join(picklepath,\n",
    "                                picklename),\n",
    "    convert_float_np64=True\n",
    ")\n",
    "\n",
    "\n",
    "print(vars(sub_class.lfp_left_INVOLUNTARY_moveboth_lidlid).keys())\n",
    "print(sub_class.lfp_left_INVOLUNTARY_moveboth_lidlid.time_arr.shape)\n",
    "print(sub_class.lfp_left_INVOLUNTARY_moveboth_lidlid.cdrs_arr.shape)\n",
    "\n",
    "print(sub_class.lfp_left_INVOLUNTARY_moveboth_lidlid.ephys_2d_arr.shape)\n",
    "print(sub_class.lfp_left_INVOLUNTARY_moveboth_lidlid.band_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = utilsFiles.load_ft_ext_cfg(FT_VERSION=='v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pac_values = fts_pac.calculate_PAC_matrix(\n",
    "    sig_pha=ssd_014.ecog_right.lo_beta,\n",
    "    sig_amp=ssd_014.ecog_right.narrow_gamma,\n",
    "    window_times=ssd_014.ecog_right.times,\n",
    "    fs=ssd_014.ecog_right.fs,\n",
    "    freq_range_pha=SETTINGS['SPECTRAL_BANDS']['lo_beta'],\n",
    "    freq_range_amp=SETTINGS['SPECTRAL_BANDS']['narrow_gamma']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importlib.reload(specFts)\n",
    "\n",
    "sel1 = 'lfp_left_REST_moveboth_lidno'\n",
    "sel2 = 'lfp_left_INVOLUNTARY_moveboth_lidlid'\n",
    "\n",
    "freqs = {'theta': [4, 8], 'beta': [12, 20],\n",
    "         'gamma': [70, 80]}\n",
    "\n",
    "# cdrs is only included for debug/analysis here\n",
    "var_list = ['beta', 'gamma', 'theta', 'cdrs']\n",
    "var_list = {v: [] for v in var_list}\n",
    "\n",
    "# get raw SSD-signals\n",
    "for sel in [sel1, sel2]:\n",
    "    var_list['theta'].extend(list(getattr(sub_class, sel).ephys_2d_arr[:, 0]))\n",
    "    var_list['beta'].extend(list(getattr(sub_class, sel).ephys_2d_arr[:, 2]))\n",
    "    var_list['gamma'].extend(list(getattr(sub_class, sel).ephys_2d_arr[:, -2]))\n",
    "    var_list['cdrs'].extend(list(getattr(sub_class, sel).cdrs_arr))\n",
    "\n",
    "# # calculate Theta based on Beta-Gamma\n",
    "# theta = specFts.get_theta_from_betaGamma(\n",
    "#     beta_sig=var_list['beta'], fs=2048,\n",
    "#     gamma_sig=var_list['gamma'],\n",
    "#     ZSCORE_envs=True,\n",
    "# )\n",
    "# var_list['theta_new'] = theta\n",
    "\n",
    "# calculate smoothed envelops for otger bands\n",
    "for v in var_list.keys():\n",
    "    if v in ['cdrs', 'theta_new']: continue\n",
    "    sig = bursts_funcs.get_envelop(\n",
    "        np.array(var_list[v]), fs=2048, bandpass_freqs=freqs[v]\n",
    "    )\n",
    "    sig = ftHelp.smoothing(sig, fs=2048, win_ms=250,)\n",
    "    var_list[v] = np.array(sig)\n",
    "\n",
    "\n",
    "# window resulting arrays\n",
    "\n",
    "var_windows = {f: [] for f in var_list}\n",
    "\n",
    "cuts = np.arange(0, len(var_list['theta']), 10*2048)\n",
    "\n",
    "for i1, i2 in zip(cuts[:-1], cuts[1:]):\n",
    "    for f in var_windows:\n",
    "        var_windows[f].append(np.mean(var_list[f][i1:i2]))\n",
    "        \n",
    "for f in var_windows: var_windows[f] = np.array(var_windows[f])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_raw = getattr(sub_class, sel).ephys_2d_arr[:, 0]\n",
    "theta_bp = bursts_funcs.bandpass(theta_raw, freqs=[4, 8], fs=2048,)\n",
    "theta_analytSig = signal.hilbert(theta_bp, )\n",
    "theta_env = bursts_funcs.get_envelop(theta_raw, fs=2048,\n",
    "                                     bandpass_freqs=[4, 8], )\n",
    "theta_phase = (np.angle(theta_analytSig) + np.pi) / (np.pi * 2)  # normed between 0 and 1\n",
    "\n",
    "\n",
    "%matplotlib qt\n",
    "plt.close()\n",
    "\n",
    "plt.plot(theta_bp, alpha=.5,)\n",
    "plt.plot(theta_analytSig, alpha=.5,)\n",
    "plt.plot(theta_phase, alpha=.5,)\n",
    "# plt.scatter(np.arange(len(theta_raw)), theta_raw, alpha=.7,)\n",
    "# plt.plot(theta_angle / np.pi)\n",
    "# plt.plot(np.angle(var_list['theta']))\n",
    "\n",
    "plt.xlim(0, 30 * 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "\n",
    "mwu_stat, mwu_p = stats.mannwhitneyu(\n",
    "    var_windows['theta_new'][var_windows['cdrs'] == 0],\n",
    "    var_windows['theta_new'][var_windows['cdrs'] > 0]\n",
    ")\n",
    "\n",
    "print(f'Mann Witney U diff: stat: {mwu_stat},  p: {round(mwu_p, 5)}')\n",
    "\n",
    "for f in ['theta', 'beta', 'gamma', 'theta_new']:\n",
    "    res_pearson = stats.pearsonr(var_windows[f], var_windows['cdrs'])\n",
    "\n",
    "    print(f'\\n\\t{f.upper()} Pearson R: '\n",
    "          f'{res_pearson},\\n...CI: {res_pearson.confidence_interval()}')\n",
    "    \n",
    "    plt.scatter(var_windows['cdrs'], var_windows[f],\n",
    "                label=f'{f}: R: {round(res_pearson[0], 2)}, '\n",
    "                f'p = {round(res_pearson[1], 6)}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop phase-difference values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_deg = phase * (180 / np.pi)\n",
    "\n",
    "phase_bins = {}\n",
    "for bin_start in np.arange(-180, 180, 20):\n",
    "    bin_sel = np.logical_and(phase_deg > bin_start,\n",
    "                             phase_deg<(bin_start+20))\n",
    "    ampl_sel = ampl[bin_sel]\n",
    "\n",
    "    phase_bins[bin_start] = ampl_sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bin in phase_bins:\n",
    "    amps = phase_bins[bin]\n",
    "    plt.hist(amps,alpha=.3)\n",
    "    ent = stats.entropy(amps)\n",
    "    plt.title(f'{bin} degree: entropy {ent}')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSD with meet toolbox (https://github.com/neurophysics/meet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import meet.meet as meet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "i_win = 5\n",
    "win_dat = data.list_mne_objects[i_win].get_data()  # epochs x channels x times\n",
    "ch_names = data.list_mne_objects[i_win].ch_names\n",
    "fs = data.info['sfreq']\n",
    "nperseg = 1024\n",
    "bw_ranges = {   'alpha': [8, 12],\n",
    "                'lo_beta': [12, 20],\n",
    "                'hi_beta': [20, 35],\n",
    "                'beta': [12, 35],\n",
    "                'midgamma': [60, 90]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lfpecog_features import feats_SSD as ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and plot SSD functionality\n",
    "\n",
    "importlib.reload(ssd)\n",
    "\n",
    "SOURCE_SEL = 'ECOG'\n",
    "F_BAND_SEL = 'midgamma'\n",
    "epoch_i = 50\n",
    "plt.close()\n",
    "# select 2d data of one source (n-channels x n-samples)\n",
    "ch_sel = [n.startswith(SOURCE_SEL) for n in ch_names]\n",
    "epoch_dat = win_dat[epoch_i, ch_sel, :]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8,8))\n",
    "\n",
    "for F_BAND_SEL in ['lo_beta', 'hi_beta', 'beta', 'midgamma']:\n",
    "    ssd_filt_data, ssd_pattern, ssd_eigvals = ssd.get_SSD_component(\n",
    "        data_2d=epoch_dat,\n",
    "        fband_interest=bw_ranges[F_BAND_SEL],\n",
    "        s_rate=fs,\n",
    "        use_freqBand_filtered=True,\n",
    "        return_comp_n=0,\n",
    "    )\n",
    "    f, psd = signal.welch(ssd_filt_data, axis=-1, nperseg=fs, fs=fs)\n",
    "\n",
    "    axes[0].plot(ssd_filt_data, label=F_BAND_SEL)\n",
    "    axes[1].plot(f, psd, label=F_BAND_SEL)\n",
    "\n",
    "# psd of origin\n",
    "for i in range(epoch_dat.shape[0]):\n",
    "    f, psd = signal.welch(epoch_dat[i, :], nperseg=fs, fs=fs)\n",
    "    axes[2].plot(f, psd, label=F_BAND_SEL, c='k', alpha=.3,)\n",
    "\n",
    "axes[0].legend(ncol=4)\n",
    "axes[1].legend()\n",
    "axes[0].set_title('SSD filtered bands', fontsize=14, weight='bold',)\n",
    "axes[0].set_ylabel('LFP (a.u.)', fontsize=14,)\n",
    "axes[0].set_xlabel('Time (samples, 2048 Hz)', fontsize=14,)\n",
    "axes[1].set_title('Freq-specific PSD after SSD', fontsize=14, weight='bold',)\n",
    "axes[2].set_title('Original PSDs of channels', fontsize=14, weight='bold',)\n",
    "\n",
    "for ax in [1, 2]:\n",
    "    axes[ax].set_xlim(0, 100)\n",
    "    axes[ax].set_xlabel('Frequency (Hz)', fontsize=14,)\n",
    "    axes[ax].set_ylabel('Power (a.u.)', fontsize=14,)\n",
    "\n",
    "for ax in axes: ax.tick_params(axis='both', labelsize=10)\n",
    "plt.tight_layout()\n",
    "figname = 'SSD_example_timeseries_PSD'\n",
    "# plt.savefig(os.path.join(figpath, 'ft_exploration', 'SSD', figname),\n",
    "#             dpi=300, facecolor='w',)\n",
    "\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot SSD Components\n",
    "# for i, b in enumerate(SSD_beta):\n",
    "#     plt.plot(b, label=f'ch {i}',\n",
    "#              alpha=(1 - (i * .2)),\n",
    "#              lw=5 - i)\n",
    "# # beta1d = SSD_beta.T @ np.atleast_2d(SSD_eigvals).T  # combined signal, not relevant\n",
    "# # plt.plot(beta1d, label='product', c='k')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_excel('C://Users/habetsj/Downloads/Mappe1.xlsx', header=1)\n",
    "\n",
    "d = d[[k for k in d.keys() if k.startswith('UPDRS')]]\n",
    "nan_sel = [~pd.isna(d.values[i]).any() for i in np.arange(d.shape[0])]\n",
    "d = d.iloc[nan_sel, :].reset_index(drop=True)\n",
    "str_sel = [any([isinstance(s, str) for s in d.iloc[i]]) for i in np.arange(d.shape[0])]\n",
    "d = d.iloc[~np.array(str_sel), :].reset_index(drop=True)\n",
    "\n",
    "plt.scatter(d.iloc[:, 1], d.iloc[:, 0])\n",
    "plt.xlabel(d.keys()[1])\n",
    "plt.ylabel(d.keys()[0])\n",
    "\n",
    "w = stats.wilcoxon(d.iloc[:, 1], d.iloc[:, 0])\n",
    "p = stats.pearsonr(d.iloc[:, 1], d.iloc[:, 0])\n",
    "plt.title(f'{p}\\n{w}\\n(n = {d.shape[0]} PD patients)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('C://Users/habetsj/Downloads/Fallzahl_OFF_ON', dpi=150, facecolor='w')\n",
    "plt.close()\n",
    "\n",
    "plt.scatter(d.iloc[:, 1], d.iloc[:, 1] - d.iloc[:, 0])\n",
    "plt.xlabel(d.keys()[1])\n",
    "plt.ylabel('UPDRS improvement due to Stimulation')\n",
    "\n",
    "w = stats.wilcoxon(d.iloc[:, 1], d.iloc[:, 1] - d.iloc[:, 0])\n",
    "p = stats.pearsonr(d.iloc[:, 1], d.iloc[:, 1] - d.iloc[:, 0])\n",
    "plt.title(f'{p}\\n{w}\\n(n = {d.shape[0]} PD patients)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('C://Users/habetsj/Downloads/Fallzahl_OFF_change', dpi=150, facecolor='w')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ecog_dysk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b948574b4cc10c9dd8fa8cab55862e7a8500229b4c7ca6593391d5001a62fb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
