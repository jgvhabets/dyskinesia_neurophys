{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to get started working with MediaPipe\n",
    "\n",
    "Goal is to use mediapipe to quantify leg and arm arm movement with the focus on dyskinetic movements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python env used:\n",
    "\n",
    "\n",
    "`conda create --name mediapipe python==3.9 numpy pandas scipy scikit-learn jupyter matplotlib`\n",
    "\n",
    "afterwards installed:\n",
    "\n",
    "`pip install mediapipe`, or when suffering Charite proxies: `pip install --proxy=http://proxy.charite.de:8080 mediapipe` (install mediapipe includes installing opencv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importing Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python and external packages\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import csv\n",
    "from dataclasses import dataclass, field, fields\n",
    "import math\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# mediapipe specific imports\n",
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_fileManagement import get_project_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tryout Posture Detection MediaPipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from OpenCV Posture Tutorial (learnopencv.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions (later to py's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDistance(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculates distance of person to camera\n",
    "    \"\"\"\n",
    "    dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAngle(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Find Angle between two 2d-coordinates in degrees\n",
    "    \"\"\"\n",
    "    theta = math.acos((y2 - y1) * (-y1) / (math.sqrt(\n",
    "        (x2 - x1)**2 + (y2 - y1)**2) * y1\n",
    "    ))\n",
    "    degree = int(180 / math.pi) * theta\n",
    "\n",
    "    return degree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and opening MP4's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_folder = os.path.join(get_project_path('data'), 'videos')\n",
    "video_fname = 'rest_example_video.MP4'\n",
    "video_path = os.path.join(video_folder, video_fname)\n",
    "os.path.exists(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully opened MP4: c:\\Users\\habetsj\\Research\\projects\\dyskinesia_neurophys\\data\\videos\\rest_example_video.MP4\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if cap.isOpened(): print(f'Succesfully opened MP4: {video_path}')\n",
    "else: raise ValueError(f'Failed to open {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out_folder = os.path.join(get_project_path('results'), 'mediapipe')\n",
    "if not os.path.exists(video_out_folder): os.makedirs(video_out_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard mediapipe fucntions to call\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_width = int(cap.get(3))\n",
    "frame_heigth = int(cap.get(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fname = 'out_test.mp4'\n",
    "out = cv2.VideoWriter(\n",
    "    os.path.join(video_out_folder, out_fname),\n",
    "    cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "    10,\n",
    "    (frame_width, frame_heigth)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mflip(image, \u001b[39m1\u001b[39m), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m      7\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m results \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[0;32m     10\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     11\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_RGB2BGR)\n",
      "File \u001b[1;32mc:\\Users\\habetsj\\Anaconda3\\envs\\mediapipe\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m   \u001b[39m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[39mfor\u001b[39;00m landmark \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mpose_landmarks\u001b[39m.\u001b[39mlandmark:  \u001b[39m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\habetsj\\Anaconda3\\envs\\mediapipe\\lib\\site-packages\\mediapipe\\python\\solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[0;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[0;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[0;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://github.com/google/mediapipe/issues/1589\n",
    "while cap.isOpened():\n",
    "\n",
    "    ret, image = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    out.write(image)\n",
    "pose.close()\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learnopencv.com/building-a-body-posture-analysis-system-using-mediapipe/\n",
    "\n",
    "good_frames = 0\n",
    "bad_frames  = 0\n",
    " \n",
    "# Font type.\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    " \n",
    "# Colors.\n",
    "blue = (255, 127, 0)\n",
    "red = (50, 50, 255)\n",
    "green = (127, 255, 0)\n",
    "dark_blue = (127, 20, 0)\n",
    "light_green = (127, 233, 100)\n",
    "yellow = (0, 255, 255)\n",
    "pink = (255, 0, 255)\n",
    " \n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully opened MP4: c:\\Users\\habetsj\\Research\\projects\\dyskinesia_neurophys\\data\\videos\\rest_example_video.MP4\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if cap.isOpened(): print(f'Succesfully opened MP4: {video_path}')\n",
    "else: raise ValueError(f'Failed to open {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta.\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_size = (width, height)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "# Video writer.\n",
    "out_fname = 'out_test.mp4'\n",
    "video_output = cv2.VideoWriter(\n",
    "    os.path.join(video_out_folder, out_fname),\n",
    "    fourcc,\n",
    "    fps,\n",
    "    frame_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture frames.\n",
    "success, image = cap.read()\n",
    "if not success:\n",
    "    print(\"Null.Frames\")\n",
    "    # break\n",
    "# Get fps.\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "# Get height and width of the frame.\n",
    "h, w = image.shape[:2]\n",
    " \n",
    "# Convert the BGR image to RGB.\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# Process the image.\n",
    "keypoints = pose.process(image)\n",
    "# Convert the image back to BGR.\n",
    "image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mediapipe.python.solution_base.SolutionOutputs"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose.process(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'SolutionOutputs' has no attribute 'pose_landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m norm_coordinate  \u001b[39m=\u001b[39m pose\u001b[39m.\u001b[39;49mprocess(image)\u001b[39m.\u001b[39;49mpose_landmark\u001b[39m.\u001b[39mlandmark[mp\u001b[39m.\u001b[39msolutions\u001b[39m.\u001b[39mpose\u001b[39m.\u001b[39mPoseLandmark\u001b[39m.\u001b[39mleft_elbow]\u001b[39m.\u001b[39mcoordinate\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'SolutionOutputs' has no attribute 'pose_landmark'"
     ]
    }
   ],
   "source": [
    "norm_coordinate  = pose.process(image).pose_landmark.landmark[mp.solutions.pose.PoseLandmark.left_elbow].coordinate \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29aedbcf2682543b52e898337fde7456e2c8d0480ec9784284a0f0b0d8985c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
