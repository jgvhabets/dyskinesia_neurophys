{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to get started working with MediaPipe\n",
    "\n",
    "Goal is to use mediapipe to quantify leg and arm arm movement with the focus on dyskinetic movements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python env used:\n",
    "\n",
    "\n",
    "`conda create --name mediapipe python==3.9 numpy pandas scipy scikit-learn jupyter matplotlib`\n",
    "\n",
    "afterwards installed:\n",
    "\n",
    "`pip install mediapipe`, or when suffering Charite proxies: `pip install --proxy=http://proxy.charite.de:8080 mediapipe` (install mediapipe includes installing opencv)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importing Packages and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python and external packages\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "import csv\n",
    "from dataclasses import dataclass, field, fields\n",
    "import math\n",
    "from itertools import compress\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "# mediapipe specific imports\n",
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.basename(os.getcwd()) == 'mediapiping': os.chdir(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils_fileManagement import get_project_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tryout Posture Detection MediaPipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from OpenCV Posture Tutorial (learnopencv.com)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions (later to py's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDistance(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculates distance of person to camera\n",
    "    \"\"\"\n",
    "    dist = np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAngle(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Find Angle between two 2d-coordinates in degrees\n",
    "    \"\"\"\n",
    "    theta = math.acos((y2 - y1) * (-y1) / (math.sqrt(\n",
    "        (x2 - x1)**2 + (y2 - y1)**2) * y1\n",
    "    ))\n",
    "    degree = int(180 / math.pi) * theta\n",
    "\n",
    "    return degree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find file and folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = os.path.join(get_project_path('data'), 'videos')\n",
    "video_fname = 'rest_example_video.MP4'\n",
    "video_path = os.path.join(video_folder, video_fname)\n",
    "os.path.exists(video_path)\n",
    "\n",
    "video_out_folder = os.path.join(video_folder, 'output')\n",
    "if not os.path.exists(video_out_folder): os.makedirs(video_out_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and opening MP4's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully opened MP4: c:\\Users\\habetsj\\Research\\projects\\dyskinesia_neurophys\\data\\videos\\rest_example_video.MP4\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if cap.isOpened(): print(f'Succesfully opened MP4: {video_path}')\n",
    "else: raise ValueError(f'Failed to open {video_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_out_folder = os.path.join(get_project_path('results'), 'mediapipe')\n",
    "if not os.path.exists(video_out_folder): os.makedirs(video_out_folder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### finding poses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set mediapipe posing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2022/03/pose-detection-in-image-using-mediapipe-library/\n",
    "\n",
    "# Initialize mediapipe pose class.\n",
    "mp_pose = mp.solutions.pose\n",
    "lmPose = mp_pose.PoseLandmark\n",
    "\n",
    "# Setup the Pose function for images - independently for the images standalone processing.\n",
    "pose_image = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Setup the Pose function for videos - for video processing.\n",
    "pose_video = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.7,\n",
    "                          min_tracking_confidence=0.7)\n",
    "\n",
    "# Initialize mediapipe drawing class - to draw the landmarks points.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPose(image, pose, draw=False, display=False, RGB=False,):\n",
    "    \n",
    "    # original_image = image.copy()\n",
    "    \n",
    "    if not RGB: image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    resultant = pose.process(image)\n",
    "\n",
    "    if resultant.pose_landmarks and draw:\n",
    "        # convert to BGR    \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        mp_drawing.draw_landmarks(image=image, landmark_list=resultant.pose_landmarks,\n",
    "                                  connections=mp_pose.POSE_CONNECTIONS,\n",
    "                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255,255,255),\n",
    "                                                                               thickness=3, circle_radius=3),\n",
    "                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(49,125,237),\n",
    "                                                                               thickness=2, circle_radius=2))\n",
    "\n",
    "    if display:\n",
    "            \n",
    "        fig, axes = plt.subplots(1, 1, figsize=[16, 8])\n",
    "        # plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Input Image\");plt.axis('off')\n",
    "        # plt.subplot(122);plt.imshow(original_image[:,:,::-1]);plt.title(\"Pose detected Image\");plt.axis('off')\n",
    "        axes.imshow(image[:,:,::-1])\n",
    "        # axes.imshow(image)\n",
    "        axes.set_title(\"Pose detected Image\")\n",
    "        axes.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        return image, resultant\n",
    "\n",
    "    else:\n",
    "        \n",
    "        return image, resultant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_between = (1130, 1200)\n",
    "out_folder = video_out_folder\n",
    "out_fname = 'poses_out.mp4'\n",
    "video_in = video_path\n",
    "\n",
    "pose = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5,\n",
    "                          min_tracking_confidence=0.5)\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture(video_in)\n",
    "\n",
    "# Check if the video was successfully loaded\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening the video file. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "w = int(cap.get(3))\n",
    "h = int(cap.get(4))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_size = (w, h)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# Video writer.\n",
    "out = cv2.VideoWriter(os.path.join(out_folder, out_fname), fourcc, fps, frame_size)\n",
    "\n",
    "# save in df\n",
    "all_landmarks = lmPose._member_names_\n",
    "lm_vars = ['x', 'y', 'z', 'visibility']\n",
    "lm_df = pd.DataFrame(columns=[f'{m}_{v}' for m in all_landmarks for v in lm_vars])\n",
    "\n",
    "count = -1\n",
    "\n",
    "while cap.isOpened():\n",
    "    count += 1\n",
    "    if count < show_between[0]: continue\n",
    "    elif count > show_between[1]: break\n",
    "\n",
    "    # read next frame\n",
    "    ret, image = cap.read()\n",
    "    if not ret: break\n",
    "    \n",
    "    # print(count)\n",
    "    # convert for pose detection\n",
    "    image = cv2.cvtColor(\n",
    "        image,\n",
    "        cv2.COLOR_BGR2RGB\n",
    "    )\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Here we will read our image from the specified path to detect the pose\n",
    "    # output = cv2.imread(image_path)\n",
    "    image, results = detectPose(image, pose, draw=True, display=False, RGB=True)\n",
    "\n",
    "    # # Convert the image back to BGR.\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    poses_incl = ['LEFT_SHOULDER', 'RIGHT_SHOULDER']\n",
    "    \n",
    "    out.write(image)\n",
    "    \n",
    "    lm_df = pd.concat([lm_df, pd.Series([[np.nan] * lm_df.shape[1]])]).reset_index(drop=True)\n",
    "    if results.pose_landmarks:\n",
    "\n",
    "        for m in all_landmarks:\n",
    "            for v in lm_vars:\n",
    "                lm_df.loc[len(lm_df) - 1][f'{m}_x'] = results.pose_landmarks.landmark[getattr(lmPose, m)].x * w\n",
    "                lm_df.loc[len(lm_df) - 1][f'{m}_y'] = results.pose_landmarks.landmark[getattr(lmPose, m)].y * h\n",
    "                lm_df.loc[len(lm_df) - 1][f'{m}_z'] = results.pose_landmarks.landmark[getattr(lmPose, m)].z\n",
    "                lm_df.loc[len(lm_df) - 1][f'{m}_viz'] = results.pose_landmarks.landmark[getattr(lmPose, m)].visibility\n",
    "\n",
    "pose.close()\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "lm_df.to_csv(os.path.join(out_folder, f'landmarks_{out_fname}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add blurry side (with video error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "def add_blurred_bg():\n",
    "    HEIGHT = 720 \n",
    "    WIDTH = 1280\n",
    "    in_file = ffmpeg.input('input.mp4')\n",
    "    probe = ffmpeg.probe('input.mp4')\n",
    "    video_stream = next((stream for stream in probe['streams'] if stream['codec_type'] == 'video'), None)\n",
    "    iw=int(video_stream['width'])\n",
    "    ih=int(video_stream['height'])\n",
    "    nw = HEIGHT*iw/ih\n",
    "    (\n",
    "        ffmpeg\n",
    "        .overlay(\n",
    "            in_file.filter('scale', WIDTH, -2).crop(0,(WIDTH*HEIGHT/nw-HEIGHT)/2,WIDTH,HEIGHT).filter('gblur', sigma=40),\n",
    "            in_file.filter('scale', -2, HEIGHT),\n",
    "            x=(WIDTH-nw)/2\n",
    "        )\n",
    "        .output(in_file.audio, 'output.mp4')\n",
    "        .run()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29aedbcf2682543b52e898337fde7456e2c8d0480ec9784284a0f0b0d8985c1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
